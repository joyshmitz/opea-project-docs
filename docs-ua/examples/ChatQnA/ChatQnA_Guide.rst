.. _ChatQnA_Guide:

Зразковий посібник з ChatQnA
############################

.. Примітка:: Цей посібник знаходиться на ранній стадії розробки і є робочим документом з вмістом заповнювачів.

Огляд
*****

Чат-боти - це широко розповсюджений варіант використання потужних можливостей чату і
міркувань великих мовних моделей (LLM). Приклад ChatQnA є відправною точкою для початку роботи розробників у просторі GenAI.
Вважайте, що це «hello world» додатків GenAI, який можна використовувати для
рішень для широких корпоративних вертикалей, як внутрішніх, так і зовнішніх.

Ціль
****

У прикладі ChatQnA використовується архітектура розширеного пошуку (RAG),
яка швидко стає галузевим стандартом для розробки чат-ботів. Вона
поєднує переваги бази знань (через векторне сховище) і генеративних
моделей, щоб зменшити кількість галюцинацій, підтримувати актуальність інформації та використовувати специфічні знання про предметну область.

RAG заповнює прогалину в знаннях, динамічно отримуючи відповідну інформацію із
зовнішніх джерел, гарантуючи, що відповіді, які генеруються, залишаються фактичними та актуальними.
Ядром цієї архітектури є векторні бази даних, які допомагають
ефективного та семантичного пошуку інформації. Ці бази даних зберігають
дані у вигляді векторів, що дозволяє RAG швидко отримувати доступ до найбільш релевантних документів або
даних на основі семантичної подібності.

Центральним елементом архітектури RAG є використання генеративної моделі, яка
відповідає за генерацію відповідей на запити користувачів. Генеративна модель
навчається на великому масиві персоналізованих і релевантних текстових даних і здатна
генерувати відповіді, схожі на людські. Розробники можуть легко замінити генеративну
модель або векторну базу даних на власні кастомні моделі або бази даних. Це дозволяє
розробникам створювати чат-ботів, пристосованих до їхніх конкретних сценаріїв використання та
вимог. Поєднуючи генеративну модель з векторною базою даних, RAG
може надавати точні та контекстно-релевантні відповіді на запити ваших користувачів.

Приклад ChatQnA розроблено як просту, але потужну демонстрацію
архітектури RAG. Це чудова відправна точка для розробників, які прагнуть
створити чат-ботів, які можуть надавати користувачам точну та актуальну інформацію.

Щоб полегшити спільний доступ до окремих сервісів між декількома додатками GenAI, використовуйте GenAI Microservices Connector (GMC) для розгортання вашого додатку. Окрім спільного використання сервісів, він також підтримує визначення послідовних, паралельних та альтернативних кроків у трубопроводі GenAI. При цьому він підтримує динамічне перемикання між моделями, що використовуються на будь-якому етапі конвеєра GenAI.  Наприклад, у трубопроводі ChatQnA за допомогою GMC можна перемикати модель, що використовується у вбудовувачі, переранжувачі та/або LLM. 
Висхідний Vanilla Kubernetes або Red Hat OpenShift Container (RHOCP) можна використовувати як з GMC, так і без нього, хоча використання з GMC надає додаткові можливості.

ChatQnA надає кілька варіантів розгортання, включаючи одновузлове
розгортання локально або в хмарному середовищі з використанням такого обладнання, як Xeon
Scalable Processors, сервери Gaudi, графічні процесори NVIDIA і навіть на комп'ютерах зі штучним інтелектом.  Він також
підтримує розгортання Kubernetes з консоллю управління GenAI Management Console (GMC) і без неї
(GMC), а також хмарні розгортання з використанням RHOCP.

Ключові деталі впровадження
***************************

Вбудовування:
  Процес перетворення запитів користувача в числові представлення називається
  вбудовуваннями (embeddings).
Векторна база даних:
  Зберігання та пошук відповідних точок даних за допомогою векторних баз даних.
Архітектура RAG:
  Використання архітектури RAG для об'єднання баз знань та генеративних
  моделей для розробки чат-ботів з релевантними та актуальними відповідями на запити.  
Великі мовні моделі (LLM):
  Навчання та використання LLM для створення відповідей.
Варіанти розгортання:
  варіанти розгортання готового до використання ChatQnA
  наприклад, розгортання на одному вузлі та розгортання на Kubernetes.

Як це працює
************

Приклади ChatQnA демонструють основний потік інформації в системі чат-ботів,
починаючи з введення користувачем і проходячи через компоненти отримання, переранжування і
генерування компонентів, що в кінцевому підсумку призводить до виводу бота.

.. figure:: images/chatqna_architecture.png
   :alt: ChatQnA Architecture Diagram

   Ця діаграма ілюструє потік інформації в системі чат-ботів,
   починаючи від введення користувачем і проходячи через пошук, аналіз і
   генерування компонентів, що в кінцевому підсумку призводить до виводу бота.

Архітектура виконує низку кроків для обробки запитів користувачів і генерування відповідей:

1. **Embedding (Вбудовування)**: Запит користувача спочатку перетворюється в числове представлення, яке називається вбудовуванням.
   Це вбудовування фіксує семантичне значення запиту і дозволяє ефективно порівнювати його з іншими вбудовуваннями.
#. **Vector Database (Векторна база даних)**: Далі вбудовування використовується для пошуку у векторній базі даних,
   яка зберігає відповідні точки даних у вигляді векторів. Векторна база даних дозволяє ефективний і
   семантичний пошук інформації на основі подібності між вбудовуванням запиту та збереженими векторами.
#. **Re-ranker (Переранжування)**: Використовує модель для ранжування знайдених даних за їхньою корисністю.
   Векторна база даних отримує найбільш релевантні дані на основі вкладеного запиту.
   Ці дані можуть включати документи, статті або будь-яку іншу релевантну інформацію, яка може допомогти генерувати точні відповіді.
#. **LLM**: Отримані точки даних потім передаються до великих мовних моделей (LLM) для подальшої обробки.
   LLM - це потужні генеративні моделі, які були були навчені на великому масиві текстових даних.
   Вони можуть генерувати людські відповіді на основі вхідних даних.
#. **Generate Response (Сформована відповідь)**: LLM генерують відповідь на основі вхідних даних і запиту користувача.
   Потім ця відповідь повертається користувачеві як відповідь чат-бота.

Очікуваний вивід
================

TBD

Матриця валідації та передумови
===============================

Дивиться :doc:`/GenAIExamples/supported_examples`

Архітектура
***********

Архітектура ChatQnA показана нижче:

.. figure:: images/chatqna_flow_chart.png
   :alt: ChatQnA Architecture Diagram

План і схема мікросервісу
=========================

Додаток або трубопровід GenAI в OPEA зазвичай складається з набору мікросервісів для створення мегасервісу, доступ до якого здійснюється через шлюз. Мікросервіс - це компонент, призначений для виконання певної функції або завдання. Мікросервіси є будівельними блоками, що пропонують основні послуги. Мікросервіси сприяють модульності, гнучкості та масштабованості системи. Мегасервіс - це архітектурна конструкція більш високого рівня, що складається з одного або декількох мікросервісів, що забезпечує можливість збирати наскрізні додатки.
Шлюз слугує інтерфейсом для доступу користувачів. Шлюз спрямовує вхідні запити до відповідних мікросервісів у межах архітектури мегасервісу. Для отримання додаткової інформації див. `GenAI Components <https://github.com/opea-project/GenAIComps>`_.

.. mermaid::

   graph LR
    subgraph ChatQnA-MegaService["ChatQnA-MegaService"]
        direction LR
        EM([Embedding 'LangChain TEI' <br>6000])
        RET([Retrieval 'LangChain Redis'<br>7000])
        RER([Rerank 'TEI'<br>8000])
       LLM([LLM 'text-generation TGI'<br>9000])
    end

    direction TB
    TEI_EM{{TEI embedding service<br>8090}}
    VDB{{Vector DB<br>8001}}
    %% Vector DB interaction
    TEI_EM -.->|d|VDB

    DP([OPEA Data Preparation<br>6007])
    LLM_gen{{TGI/vLLM/ollama Service}}

    direction TB
    RER([OPEA Reranking<br>8000])
    TEI_RER{{TEI Reranking service<br>8808}}

    subgraph User Interface
        direction TB
        a[User Input Query]
        Ingest[Ingest data]
        UI[UI server<br>Port: 5173]
    end

    subgraph ChatQnA GateWay
        direction LR
        GW[ChatQnA GateWay<br>Port: 8888]
    end

    %% Data Preparation flow
    %% Ingest data flow
    direction LR
    Ingest[Ingest data] -->|a| UI
    UI -->|b| DP
    DP -.->|c| TEI_EM

    %% Questions interaction
    direction LR
    a[User Input Query] -->|1| UI
    UI -->|2| GW
    GW ==>|3| ChatQnA-MegaService
    EM ==>|4| RET
    RET ==>|5| RER
    RER ==>|6| LLM


    %% Embedding service flow
    direction TB
    EM -.->|3'| TEI_EM
    RET -.->|4'| TEI_EM
    RER -.->|5'| TEI_RER
    LLM -.->|6'| LLM_gen

    subgraph Legend
        X([Microservice])
        Y{{Service from industry peers}}
        Z[Gateway]
    end

Додаток або трубопровід GenAI в OPEA зазвичай складається з набору мікросервісів для створення мегасервісу, доступ до якого здійснюється через шлюз. Мікросервіс - це компонент, призначений для виконання певної функції або завдання. Мікросервіси є будівельними блоками, що пропонують основні послуги. Мікросервіси сприяють модульності, гнучкості та масштабованості системи. Мегасервіс - це архітектурна конструкція більш високого рівня, що складається з одного або декількох мікросервісів, що забезпечує можливість збирати наскрізні додатки.
Шлюз слугує інтерфейсом для доступу користувачів. Шлюз спрямовує вхідні запити до відповідних мікросервісів у межах архітектури мегасервісу. Для отримання додаткової інформації див. `GenAI Components <https://github.com/opea-project/GenAIComps>`_.

Розгортання
***********

З наведених нижче варіантів розгортання виберіть той, який найкраще відповідає вашим вимогам:

Одиночний вузол
===============

.. toctree::
   :maxdepth: 1
   
   Xeon Scalable Processor <deploy/xeon>
   Gaudi AI Accelerator <deploy/gaudi>
   Nvidia GPU <deploy/nvidia>
   AI PC <deploy/aipc>
   
Kubernetes
==========

* Xeon & Gaudi with GMC
* Xeon & Gaudi without GMC
* Using Helm Charts

Місцева хмара
=============

* Red Hat OpenShift Container Platform (RHOCP)

Вирішення проблем
*****************

TDB.

Моніторинг
**********

Тепер, коли ви розгорнули приклад ChatQnA, давайте поговоримо про моніторинг продуктивності мікросервісів у трубопроводі ChatQnA.

Моніторинг продуктивності мікросервісів має вирішальне значення для забезпечення безперебійної роботи генеративних систем ШІ. Відстежуючи такі показники, як затримка та пропускна здатність, ви можете визначити вузькі місця, виявити аномалії та оптимізувати роботу окремих мікросервісів. Це дозволяє нам проактивно вирішувати будь-які проблеми і гарантувати, що трубопровід ChatQnA працює ефективно.

Цей документ допоможе вам зрозуміти, як відстежувати в реальному часі затримку, пропускну здатність та інші показники різних мікросервісів. Ви будете використовувати **Prometheus** і **Grafana**, інструменти з відкритим вихідним кодом, для збору метрик та їх візуалізації на інформаційній панелі.

Налаштування сервера Prometheus
===============================

Prometheus - це інструмент для запису метрик в режимі реального часу, спеціально розроблений для моніторингу мікросервісів і оповіщення на основі їхніх метрик.

Кінцева точка `/metrics` на порту, на якому запущено кожен мікросервіс, виводить метрики у форматі Prometheus. Сервер Prometheus зчитує ці метрики і зберігає їх у своїй базі даних часових рядів. Наприклад, метрики для сервісу Text Generation Interface (TGI) доступні за адресою:

.. code-block:: bash

   http://${host_ip}:9009/metrics

Налаштування сервера Prometheus:

1. Завантажити Prometheus:
   Завантажте Prometheus v2.52.0 з офіційного сайту та розпакуйте файли:

.. code-block:: bash

   wget https://github.com/prometheus/prometheus/releases/download/v2.52.0/prometheus-2.52.0.linux-amd64.tar.gz
   tar -xvzf prometheus-2.52.0.linux-amd64.tar.gz

2. Налаштуйте Prometheus:
   Змініть каталог на папку Prometheus:

.. code-block:: bash

   cd prometheus-2.52.0.linux-amd64

Відредагуйте файл `prometheus.yml:

.. code-block:: bash

   vim prometheus.yml

Змініть ``job_name`` на ім'я мікросервісу, який ви хочете відстежувати. Також змініть ``targets`` на кінцеву точку завдання цього мікросервісу. Переконайтеся, що сервіс запущено, порт відкрито, і що він показує метрики, які відповідають домовленості Prometheus, у кінцевій точці ``/metrics``.

Ось приклад експорту даних метрик з мікросервісу TGI до Prometheus:

.. code-block:: yaml

   # Конфігурація сканування, що містить рівно одну кінцеву точку для сканування:
   # Ось сам Прометей.
   scrape_configs:
     # Назва завдання додається у вигляді мітки `job=<job_name>` до будь-якого часового ряду, витягнутого з цієї конфігурації.
     - job_name: "tgi"

       # metrics_path defaults to '/metrics'
       # scheme defaults to 'http'.

       static_configs:
         - targets: ["localhost:9009"]

Ось ще один приклад експорту даних метрик з мікросервісу TGI (всередині кластера Kubernetes) до Prometheus:

.. code-block:: yaml

   scrape_configs:
     - job_name: "tgi"
   
       static_configs:
         - targets: ["llm-dependency-svc.default.svc.cluster.local:9009"]

3. Запустіть сервер Prometheus:
Запустіть сервер Prometheus, не зупиняючи процес:

.. code-block:: bash
   nohup ./prometheus --config.file=./prometheus.yml &

4. Увійдіть до інтерфейсу користувача Prometheus
   Отримати доступ до інтерфейсу користувача Prometheus можна за наступною адресою:

.. code-block:: bash

   http://localhost:9090/targets?search=

>Примітка: Перед запуском Prometheus переконайтеся, що на вказаному порту (за замовчуванням 9090) не запущено жодних інших процесів. Інакше Prometheus не зможе зібрати метрики.

В інтерфейсі Prometheus ви можете бачити стан цілей і метрик, які вилучаються. Ви можете шукати змінну метрики, ввівши її в рядок пошуку.

З показниками TGI можна ознайомитися за посиланням:

.. code-block:: bash

   http://${host_ip}:9009/metrics 

Налаштування Панелі Grafana
===========================

Grafana - це інструмент для візуалізації метрик і створення панелей моніторингу. З його допомогою можна створювати кастомні панелі, які відображають метрики, зібрані Prometheus.

Щоб налаштувати панель Grafana, виконайте такі дії:

1. Завантажте Grafana:
   Завантажте Grafana v8.0.6 з офіційного сайту та розпакуйте файли:

.. code-block:: bash

   wget https://dl.grafana.com/oss/release/grafana-11.0.0.linux-amd64.tar.gz
   tar -zxvf grafana-11.0.0.linux-amd64.tar.gz

Для отримання додаткових інструкцій дивіться повну версію `Grafana installation instructions  <https://grafana.com/docs/grafana/latest/setup-grafana/installation/>`_.

2. Запустіть сервер Grafana:
   Змініть каталог на папку Grafana:

.. code-block:: bash

   cd grafana-11.0.0

Запустіть сервер Grafana, не зупиняючи процес:

.. code-block:: bash

   nohup ./bin/grafana-server &

3. Увійдіть до інтерфейсу панелі Grafana:
   У вашому браузері відкрийте інтерфейс панелі Grafana за наступною адресою:

.. code-block:: bash

   http://localhost:3000

>Примітка: Перед запуском Grafana переконайтеся, що на порту 3000 не запущено жодних інших процесів.

Увійдіть до Grafana, використовуючи облікові дані за замовчуванням:

.. code-block:: 

   username: admin
   password: admin

4. Додайте Prometheus як джерело даних:
   Вам потрібно налаштувати джерело даних, з якого Grafana буде отримувати дані. Натисніть кнопку «Джерело даних», виберіть Prometheus і вкажіть URL-адресу Prometheus ``http://localhost:9090``.

   Потім вам потрібно завантажити JSON-файл для конфігурації дашборду. Ви можете завантажити його в інтерфейсі Grafana в розділі «Home > Dashboards > Import dashboard». Зразок JSON-файлу підтримується тут: `tgi_grafana.json <https://github.com/huggingface/text-generation-inference/blob/main/assets/tgi_grafana.json>`_

5. Перегляньте інформаційну панель:
   Нарешті, відкрийте інформаційну панель в інтерфейсі Grafana, і ви побачите різні панелі, що відображають дані метрик.

   На прикладі мікросервісу TGI можна побачити наступні метрики:
   * Час до першого токена
   * Затримка декодування на токен
   * Пропускна здатність (згенеровані токени/сек)
   * Кількість токенів на запит
   * Кількість згенерованих токенів на запит

   Ви також можете відстежувати вхідні запити до мікросервісу, час відповіді на токен, тощо, в режимі реального часу.

Підсумок і наступні кроки
=========================

TBD