.. _ChatQnA_deploy_xeon:


Розгортання на одному вузлі: Масштабовані процесори XEON
########################################################

наприклад, варіант використання:
Повинен надавати контекст для вибору між vLLM та TGI.

.. tabs::

   .. tab:: Розгортання за допомогою Docker compose з vLLM

      Зробити: У розділі має бути описано, як можна реалізувати вищезгадані архіви
      за допомогою режиму vllm або обраної моделі обслуговування. Покажіть базовий кінцевий випадок E2E
      налаштувати 1 тип БД для, наприклад, Redis на основі того, що вже описано у
      chatqna (інші приклади можуть бути названі або на них можна посилатися відповідно),
      Показати, як використовувати одну модель SOTA, для llama3 та інших, з прикладом
      конфігурації. Результат використання повинен бути продемонстрований на реальному прикладі використання, що показує
      як продуктивність, так і ефективність. Для узгодженості, давайте використовувати OPEA
      документацію для сценаріїв використання RAG

      Приклади назв:

      1. Огляд
         Розкажіть кількома рядками про те, що очікується в цьому підручнику.
         Наприклад, використання Redis db та запуск моделі llama3 для демонстрації сценарію
         використання e2e з використанням OPEA та vllm.
      #. Передумови
         Включає клонування репозиторіїв, витягування необхідних контейнерів, якщо вони (UI, трубопровід і т.д.),
         встановлення змінних env, таких як проксі, отримання доступу до ваг моделі, отримання токенів на hf, lg і т.д. перевірки, якщо потрібно. І т.д.
      #. Підготовка (створення / витягування) докер-образів
         a) Цей крок передбачає створення/витягування (можливо, в майбутньому) відповідних образів докерів з покроковим описом процесу, а також перевірку на здоровий глузд в кінці
         #) Якщо потрібна кастомізація, ми покажемо 1 випадок, як це зробити

      #. Налаштування кейсів використання

         У цьому розділі буде описано, як отримати дані та інші  необхідні залежності, після чого буде показано готовність оточення мікросервісу.
         Використовуйте цей розділ, щоб також поговорити про те, як встановити інші моделі, якщо це необхідно, як  використовувати інші бази даних тощо

      #. Розгортання сценарію використання chatqna на основі docker_compose

         Це має охоплювати кроки, пов'язані із запуском мікро- і мегасервісів, а також пояснити деякі ключові моменти з того, що описано
         в докер-компіляторі. За необхідності включіть перевірку на здоровий глузд. Кожна
         команду запуску мікросервісу/мегасервісу разом з тим, що вона робить і
         очікуваним результатом буде добре додати

      #. Взаємодія з розгортанням ChatQnA. (або навігація робочим процесом chatqna)

         У цьому розділі ви дізнаєтеся, як використовувати іншу машину для взаємодії та
         валідації мікросервісу, а також про те, як переміщатися по кожному   сервісу. Наприклад, як завантажити локальний документ для підготовки даних і як отримати
         відповіді? Клієнт буде зацікавлений в отриманні результатів на запит,
         а також виміряти якість моделі та метрики досконалості (
         життя та статистика також повинні бути охоплені). Будь ласка, перевірте, чи ці дані також можуть бути згорнуті в кінцевих точках. Чи доступне завантаження шаблонів
         вже доступне? Кастомний шаблон доступний сьогодні

         Показати всі доступні кастомізації та функції

      #. Додаткові можливості (за бажанням)
         Викликати специфічні функції для конкретного випадку

      #. Запуск інтерфейсу користувача
         Покажемо кроки запуску інтерфейсу користувача та приклад скріншоту запиту і результату

   .. tab:: Розгортання за допомогою docker compose з TGI

      Цей розділ буде схожий на vLLM.  Варто спробувати використовувати єдине джерело.
